asyncapi: '2.4.0'
id: 'urn:specmesh:examples:schema_demo'
info:
  title: Protobuf demo app
  version: '1.0.0'
  description: |
    Example data-product where topics are provisioned with protobuf schemas
  license:
    name: Apache 2.0
    url: 'https://www.apache.org/licenses/LICENSE-2.0'
servers:
  test:
    url: test.mykafkacluster.org:8092
    protocol: kafka-secure
    description: Test broker
channels:
  # PRODUCER/OWNER build pipe will provision topics and publish schema to SR
  _public/user_checkout:
    # publish bindings to instruct topic configuration per environment
    bindings:
      kafka:
        envs:
          - staging
          - prod
        partitions: 3
        replicas: 1
        retention: 1
        configs:
          # use https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html
          cleanup.policy: delete

    publish:
      summary: User purchase confirmation
      operationId: onUserCheckout
      message:
        bindings:
          kafka:
            schemaIdLocation: "payload"
            schemaIdPayloadEncoding: "confluent"
        schemaFormat: "application/proto;version=1.9.0"
        contentType: "application/proto"
        payload:
          $ref: "/user_checkout.proto"


  # PRODUCER/OWNER build pipe will publish schema to SR
  _public/user_info:
    # publish bindings to instruct topic configuration per environment
    bindings:
      kafka:
        envs:
          - staging
          - prod
        partitions: 3
        replicas: 1
        retention: 1
        configs:
          cleanup.policy: delete

    publish:
      summary: User purchase confirmation
      operationId: onUserCheckout
      message:
        bindings:
          kafka:
            schemaIdLocation: "payload"
            schemaIdPayloadEncoding: "confluent"
        schemaFormat: "application/proto;version=1.9.0"
        contentType: "application/octet-stream"
        payload:
          $ref: "/user_info.proto"
  # PRODUCER/OWNER build pipe will publish schema to SR
  _public/user_info_enriched:
    # publish bindings to instruct topic configuration per environment
    bindings:
      kafka:
        envs:
          - staging
          - prod
        partitions: 3
        replicas: 1
        retention: 1
        configs:
          cleanup.policy: delete

    publish:
      summary: User purchase confirmation with address
      operationId: onUserCheckout
      message:
        bindings:
          kafka:
            schemaIdLocation: "payload"
            schemaIdPayloadEncoding: "confluent"
        schemaFormat: "application/avro;version=1.9.0"
        contentType: "application/avro"
        payload:
          $ref: "/user_info_enriched.proto"
  # SUBSCRIBER WILL REQUEST SCHEMA from SR and CodeGen required classes. Header will be used for Id
  /london/hammersmith/transport/_public/tube:
    subscribe:
      summary: Humans arriving in the borough
      bindings:
        kafka:
          # should prefixed with the domain id
          groupId: 'aConsumerGroupId'
      message:
        schemaFormat: "application/vnd.apache.avro+json;version=1.9.0"
        contentType: "application/octet-stream"
        bindings:
          kafka:
            schemaIdLocation: "payload"
            schemaIdPayloadEncoding: "confluent"
            key:
              type: string
        payload:
          # client should lookup this schema remotely from the schema registry - it is owned by a different domain publisher
          $ref: "london.hammersmith.transport._public.tube.passenger.avsc"

